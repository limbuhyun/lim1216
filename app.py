import os
import json
import re
from dataclasses import dataclass
from typing import Optional, Dict, Any, Tuple

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt

# Optional: OpenAI (only used if key is provided)
try:
    from openai import OpenAI
except Exception:
    OpenAI = None


# -----------------------------
# App config
# -----------------------------
st.set_page_config(page_title="í™˜ê²½ ë°ì´í„° ë¶„ì„ (AI í¬í•¨)", layout="wide")

st.title("ğŸŒ í™˜ê²½ ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ (Streamlit + OpenAI API)")
st.caption("ì—…ë¡œë“œ ë°ì´í„° ìë™ EDA + í†µê³„ ë¶„ì„ + (ì„ íƒ) AI êµ¬ì¡°í™” ì¸ì‚¬ì´íŠ¸/ë³´ê³ ì„œ ìƒì„±")

# -----------------------------
# Utilities
# -----------------------------
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE_RE = re.compile(r"(\+?\d{1,3}[-\s]?)?(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4})")

def mask_pii_text(s: str) -> str:
    s = EMAIL_RE.sub("[EMAIL]", s)
    s = PHONE_RE.sub("[PHONE]", s)
    return s

def mask_pii_df(df: pd.DataFrame, max_cells: int = 20000) -> pd.DataFrame:
    out = df.copy()
    # Cost/latency guard
    if out.size > max_cells:
        out = out.head(max(50, min(500, len(out))))
    obj_cols = [c for c in out.columns if out[c].dtype == "object"]
    for c in obj_cols:
        out[c] = out[c].astype(str).map(mask_pii_text)
    return out

def safe_get_secret(key: str, default: Optional[str] = None) -> Optional[str]:
    if key in st.secrets:
        return st.secrets.get(key, default)
    return os.getenv(key, default)

@st.cache_resource
def get_openai_client(api_key: str):
    if OpenAI is None:
        raise RuntimeError("openai íŒ¨í‚¤ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. requirements.txt ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    return OpenAI(api_key=api_key)

@st.cache_data
def load_csv(file) -> pd.DataFrame:
    return pd.read_csv(file)

@st.cache_data
def load_excel(file) -> pd.DataFrame:
    return pd.read_excel(file)

def infer_datetime(df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[str]]:
    """
    Try to create a datetime column from common patterns.
    Returns (df, datetime_col_name or None).
    """
    out = df.copy()
    # If already has a datetime-like column
    for c in out.columns:
        if np.issubdtype(out[c].dtype, np.datetime64):
            return out, c

    # Common patterns: Year/Month/Day
    cols = {c.lower(): c for c in out.columns}
    if {"year", "month", "day"}.issubset(cols.keys()):
        y, m, d = cols["year"], cols["month"], cols["day"]
        out["date"] = pd.to_datetime(dict(year=out[y], month=out[m], day=out[d]), errors="coerce")
        return out, "date"

    # Single date column named like date/time
    for key in ["date", "datetime", "time", "timestamp"]:
        if key in cols:
            c = cols[key]
            out[c] = pd.to_datetime(out[c], errors="coerce")
            return out, c

    return out, None

def numeric_cols(df: pd.DataFrame):
    return df.select_dtypes(include=[np.number]).columns.tolist()

def make_eda(df: pd.DataFrame) -> Dict[str, Any]:
    eda = {}
    eda["shape"] = list(df.shape)
    eda["columns"] = []
    for c in df.columns:
        eda["columns"].append({
            "name": c,
            "dtype": str(df[c].dtype),
            "missing": int(df[c].isna().sum()),
            "n_unique": int(df[c].nunique(dropna=True))
        })
    num = numeric_cols(df)
    if num:
        desc = df[num].describe().T
        eda["numeric_describe"] = desc[["count","mean","std","min","25%","50%","75%","max"]].replace({np.nan: None}).to_dict()
    else:
        eda["numeric_describe"] = {}

    cat = [c for c in df.columns if df[c].dtype == "object"]
    top = {}
    for c in cat[:30]:
        vc = df[c].astype(str).value_counts(dropna=False).head(12)
        top[c] = [{"value": str(i), "count": int(vc[i])} for i in vc.index]
    eda["top_categories"] = top
    return eda

def plot_time_series(df: pd.DataFrame, date_col: str, y_col: str, group_col: Optional[str] = None):
    fig, ax = plt.subplots()
    if group_col and group_col in df.columns:
        for g, gdf in df.sort_values(date_col).groupby(group_col):
            ax.plot(gdf[date_col], gdf[y_col], label=str(g))
        ax.legend()
    else:
        d = df.sort_values(date_col)
        ax.plot(d[date_col], d[y_col])
    ax.set_xlabel(date_col)
    ax.set_ylabel(y_col)
    ax.set_title(f"{y_col} over time")
    st.pyplot(fig, clear_figure=True)

def plot_monthly_climatology(df: pd.DataFrame, date_col: str, y_col: str, group_col: Optional[str] = None):
    d = df.dropna(subset=[date_col, y_col]).copy()
    d["month"] = pd.to_datetime(d[date_col]).dt.month
    fig, ax = plt.subplots()
    if group_col and group_col in d.columns:
        for g, gdf in d.groupby(group_col):
            m = gdf.groupby("month")[y_col].mean()
            ax.plot(m.index, m.values, label=str(g))
        ax.legend()
    else:
        m = d.groupby("month")[y_col].mean()
        ax.plot(m.index, m.values)
    ax.set_xlabel("month")
    ax.set_ylabel(f"mean({y_col})")
    ax.set_title("Monthly climatology (mean by month)")
    st.pyplot(fig, clear_figure=True)

def compute_anomaly(df: pd.DataFrame, date_col: str, y_col: str, group_col: Optional[str] = None) -> pd.DataFrame:
    d = df.dropna(subset=[date_col, y_col]).copy()
    d["month"] = pd.to_datetime(d[date_col]).dt.month
    if group_col and group_col in d.columns:
        clim = d.groupby([group_col, "month"])[y_col].mean().rename("clim").reset_index()
        out = d.merge(clim, on=[group_col, "month"], how="left")
        out["anomaly"] = out[y_col] - out["clim"]
    else:
        clim = d.groupby("month")[y_col].mean().rename("clim").reset_index()
        out = d.merge(clim, on="month", how="left")
        out["anomaly"] = out[y_col] - out["clim"]
    return out

# -----------------------------
# OpenAI Structured Output
# -----------------------------
INSIGHT_SCHEMA = {
  "name": "environment_analysis_report",
  "schema": {
    "type": "object",
    "additionalProperties": False,
    "properties": {
      "one_line_summary": {"type": "string"},
      "key_findings": {
        "type": "array",
        "items": {
          "type": "object",
          "additionalProperties": False,
          "properties": {
            "title": {"type": "string"},
            "evidence": {"type": "string"},
            "impact": {"type": "string"},
            "next_step": {"type": "string"}
          },
          "required": ["title", "evidence", "impact", "next_step"]
        }
      },
      "data_quality_warnings": {"type": "array", "items": {"type": "string"}},
      "statistical_notes": {"type": "array", "items": {"type": "string"}},
      "recommended_models": {
        "type": "array",
        "items": {
          "type": "object",
          "additionalProperties": False,
          "properties": {
            "model": {"type": "string"},
            "why": {"type": "string"},
            "how": {"type": "string"}
          },
          "required": ["model", "why", "how"]
        }
      },
      "executive_report_md": {"type": "string"}
    },
    "required": ["one_line_summary", "key_findings", "data_quality_warnings", "statistical_notes", "recommended_models", "executive_report_md"]
  }
}

def ai_report(
    df: pd.DataFrame,
    eda: Dict[str, Any],
    domain_context: str,
    date_col: Optional[str],
    y_col: Optional[str],
    group_col: Optional[str],
    model_name: str,
    api_key: str,
    user_requirements: str
) -> Dict[str, Any]:
    """
    Sends ONLY masked sample rows + summary stats to the model.
    """
    masked = mask_pii_df(df)
    sample = masked.sample(min(40, len(masked)), random_state=42).to_dict(orient="records")

    payload = {
        "domain_context": domain_context,
        "user_requirements": user_requirements,
        "dataset_shape": eda.get("shape"),
        "columns": eda.get("columns"),
        "numeric_describe": eda.get("numeric_describe"),
        "top_categories": eda.get("top_categories"),
        "selected_datetime_col": date_col,
        "selected_target_col": y_col,
        "selected_group_col": group_col,
        "masked_sample_rows": sample
    }

    system = (
        "ë‹¹ì‹ ì€ í™˜ê²½ ë°ì´í„°(ê¸°í›„/í•´ì–‘/ëŒ€ê¸°/ì„¤ë¬¸ í¬í•¨) ë¶„ì„ì„ ì´ê´„í•˜ëŠ” ìˆ˜ì„ ë°ì´í„° ê³¼í•™ìì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìê°€ ì œê³µí•œ user_requirementsë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì¤€ìˆ˜í•˜ì„¸ìš”. "
        "ê³¼ì¥ ê¸ˆì§€. ê´€ì¸¡/í‘œë³¸/ì¸¡ì •ì˜ í•œê³„ë¥¼ ëª…í™•íˆ ì§€ì í•˜ê³ , í†µê³„ì  í•¨ì •(ìƒê´€=ì¸ê³¼, ê³„ì ˆì„±, ìê¸°ìƒê´€, ì´ìƒì¹˜, ê²°ì¸¡, í‘œë³¸ í¸í–¥)ì„ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”. "
        "ì¶œë ¥ì€ ë°˜ë“œì‹œ ì£¼ì–´ì§„ JSON Schemaë¥¼ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤. "
        "executive_report_mdëŠ” 1~2í˜ì´ì§€ ë¶„ëŸ‰ì˜ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œë¡œ ì‘ì„±í•˜ì„¸ìš”(ì œëª©/ìš”ì•½/í•µì‹¬ ê²°ê³¼/ê¶Œê³ /í•œê³„)."
    )

    client = get_openai_client(api_key)

    # ì‚¬ìš©ì ìš”êµ¬ ì‚¬í•­ ìš°ì„  ì¤€ìˆ˜
 (openai>=1.40 ê¶Œì¥)
    resp = client.responses.create(
        model=model_name,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)}
        ],
        text={
            "format": {
                "type": "json_schema",
                "json_schema": INSIGHT_SCHEMA
            }
        }
    )

    # Prefer output_text when available
    out_text = getattr(resp, "output_text", None)
    if not out_text:
        # Fallback: try to find text in resp.model_dump()
        dump = resp.model_dump()
        out_text = json.dumps(dump, ensure_ascii=False)

    return json.loads(out_text)

# -----------------------------
# Sidebar controls
# -----------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    domain_context = st.text_area(
        "ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸(ë¶„ì„ ëª©ì /ë°°ê²½)",
        value="í•´ë¹™(Sea Ice) ë˜ëŠ” í™˜ê²½ ê´€ì¸¡ ë°ì´í„°ì˜ ì¥ê¸° ì¶”ì„¸/ê³„ì ˆì„±/ë³€ë™ì„± ë¶„ì„",
        height=110
    )

    
    st.subheader("ìš”êµ¬ ì‚¬í•­(ë¶„ì„/ë³´ê³ ì„œ ì§€ì‹œ)")
    user_requirements = st.text_area(
        "ìš”êµ¬ ì‚¬í•­ì„ ììœ ë¡­ê²Œ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë°˜ë“œì‹œ í¬í•¨í•  ì§€í‘œ/í‘œ/ê·¸ë˜í”„/í•´ì„ ê´€ì /í†¤)",
        value=(
            "- ê²°ê³¼ëŠ” ì •ì±… ì œì–¸ ì¤‘ì‹¬ìœ¼ë¡œ\n"
            "- í‘œë³¸ í¸í–¥/ê²°ì¸¡ ì²˜ë¦¬/ê³„ì ˆì„±/ìê¸°ìƒê´€ì„ ë°˜ë“œì‹œ ì–¸ê¸‰\n"
            "- í•µì‹¬ ê·¸ë˜í”„ 3ê°œ(ì‹œê³„ì—´, ì›”ë³„ climatology, anomaly) í•´ì„ í¬í•¨\n"
            "- ê²°ë¡ ì€ 5ì¤„ ì´ë‚´ ìš”ì•½ + ë‹¤ìŒ ì•¡ì…˜ 3ê°œ"
        ),
        height=170
    )
st.subheader("OpenAI (ì„ íƒ)")
    default_model = safe_get_secret("OPENAI_MODEL", "gpt-4.1-mini")
    model_name = st.text_input("ëª¨ë¸", value=default_model)
    api_key = safe_get_secret("OPENAI_API_KEY", "")
    api_key = st.text_input("OPENAI_API_KEY", value=api_key, type="password", help="Streamlit secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ê¶Œì¥")
    enable_ai = st.toggle("AI ì¸ì‚¬ì´íŠ¸ ì‚¬ìš©", value=bool(api_key))

    st.divider()
    st.subheader("ë°ì´í„°")
    uploaded = st.file_uploader("CSV/Excel ì—…ë¡œë“œ", type=["csv", "xlsx", "xls"])
    use_sample = st.checkbox("ìƒ˜í”Œ ë°ì´í„°(ë¶ê·¹/ë‚¨ê·¹ í•´ë¹™) ì‚¬ìš©", value=(uploaded is None))

# -----------------------------
# Load data
# -----------------------------
df = None
if uploaded is not None:
    name = uploaded.name.lower()
    if name.endswith(".csv"):
        df = load_csv(uploaded)
    else:
        df = load_excel(uploaded)
elif use_sample:
    # packaged sample (seaice.csv) should be next to this file when deployed
    sample_path = os.path.join(os.path.dirname(__file__), "seaice.csv")
    if os.path.exists(sample_path):
        df = pd.read_csv(sample_path)
    else:
        st.warning("ìƒ˜í”Œ seaice.csvê°€ ì•± í´ë”ì— ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ì—…ë¡œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")

if df is None:
    st.stop()

# Infer datetime
df, dt_col = infer_datetime(df)

# -----------------------------
# Basic EDA
# -----------------------------
st.subheader("1) ë°ì´í„° ìš”ì•½ (EDA)")
eda = make_eda(df)

c1, c2, c3 = st.columns(3)
with c1:
    st.metric("í–‰", eda["shape"][0])
with c2:
    st.metric("ì—´", eda["shape"][1])
with c3:
    miss_total = sum(x["missing"] for x in eda["columns"])
    st.metric("ê²°ì¸¡ì¹˜ ì´í•©", miss_total)

with st.expander("ì—´ ë©”íƒ€ì •ë³´(ê²°ì¸¡/ìœ í˜•/ê³ ìœ ê°’)"):
    st.dataframe(pd.DataFrame(eda["columns"]).sort_values("missing", ascending=False), use_container_width=True)

if st.checkbox("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", value=True):
    st.dataframe(df.head(50), use_container_width=True)

# -----------------------------
# Analysis controls
# -----------------------------
st.subheader("2) ë¶„ì„ ì„¤ì •")

num = numeric_cols(df)
all_cols = df.columns.tolist()

colA, colB, colC = st.columns([1,1,1])
with colA:
    date_col = st.selectbox("ì‹œê°„ ì»¬ëŸ¼", options=[None] + all_cols, index=(1 if dt_col in all_cols else 0))
with colB:
    y_col = st.selectbox("ë¶„ì„ ëŒ€ìƒ(ìˆ˜ì¹˜í˜•)", options=[None] + num, index=(1 if len(num) else 0))
with colC:
    possible_groups = [c for c in all_cols if df[c].dtype == "object"][:30]
    group_col = st.selectbox("ê·¸ë£¹ ì»¬ëŸ¼(ì„ íƒ)", options=[None] + possible_groups, index=(1 if "hemisphere" in possible_groups else 0))

dff = df.copy()
if date_col and date_col in dff.columns:
    dff = dff.dropna(subset=[date_col])
    dff[date_col] = pd.to_datetime(dff[date_col], errors="coerce")
    dff = dff.dropna(subset=[date_col])

    min_d, max_d = dff[date_col].min(), dff[date_col].max()
    rng = st.slider("ê¸°ê°„ í•„í„°", min_value=min_d.to_pydatetime(), max_value=max_d.to_pydatetime(),
                    value=(min_d.to_pydatetime(), max_d.to_pydatetime()))
    dff = dff[(dff[date_col] >= pd.Timestamp(rng[0])) & (dff[date_col] <= pd.Timestamp(rng[1]))]

if group_col and group_col in dff.columns:
    groups = sorted(dff[group_col].dropna().astype(str).unique().tolist())
    picked = st.multiselect("ê·¸ë£¹ ì„ íƒ", options=groups, default=groups[:min(len(groups), 6)])
    if picked:
        dff = dff[dff[group_col].astype(str).isin(picked)]

# -----------------------------
# Core plots
# -----------------------------
st.subheader("3) ì‹œê°í™” & í•µì‹¬ í†µê³„")

if date_col and y_col:
    plot_time_series(dff, date_col, y_col, group_col=group_col)
    plot_monthly_climatology(dff, date_col, y_col, group_col=group_col)

    st.markdown("**ì›”ë³„ ê¸°ì¤€ì„ (climatology) ëŒ€ë¹„ ì´ìƒ(anomaly)**")
    anom = compute_anomaly(dff, date_col, y_col, group_col=group_col)

    fig, ax = plt.subplots()
    if group_col and group_col in anom.columns:
        for g, gdf in anom.sort_values(date_col).groupby(group_col):
            ax.plot(gdf[date_col], gdf["anomaly"], label=str(g))
        ax.legend()
    else:
        ad = anom.sort_values(date_col)
        ax.plot(ad[date_col], ad["anomaly"])
    ax.axhline(0)
    ax.set_xlabel(date_col)
    ax.set_ylabel("anomaly")
    ax.set_title("Anomaly over time (de-seasonalized)")
    st.pyplot(fig, clear_figure=True)

    # Trend (simple) - annual aggregation helps reduce autocorrelation/seasonality
    st.markdown("**ì—°ë„ë³„ ì§‘ê³„ + ë‹¨ìˆœ ì¶”ì„¸(ì„ í˜•íšŒê·€) ì°¸ê³ **")
    anom["year"] = pd.to_datetime(anom[date_col]).dt.year
    if group_col and group_col in anom.columns:
        rows = []
        for g, gdf in anom.groupby(group_col):
            annual = gdf.groupby("year")[y_col].mean().dropna()
            if len(annual) >= 5:
                x = annual.index.values.astype(float)
                y = annual.values.astype(float)
                # simple OLS
                b1, b0 = np.polyfit(x, y, 1)
                rows.append({"group": str(g), "years": int(len(annual)), "slope_per_year": float(b1), "intercept": float(b0)})
        if rows:
            st.dataframe(pd.DataFrame(rows).sort_values("slope_per_year"), use_container_width=True)
        else:
            st.info("ì—°ë„ë³„ ì§‘ê³„ í›„ ì¶”ì„¸ ê³„ì‚°ì— í•„ìš”í•œ í‘œë³¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    else:
        annual = anom.groupby("year")[y_col].mean().dropna()
        if len(annual) >= 5:
            x = annual.index.values.astype(float)
            y = annual.values.astype(float)
            b1, b0 = np.polyfit(x, y, 1)

            fig, ax = plt.subplots()
            ax.plot(annual.index, annual.values)
            ax.plot(annual.index, b1 * annual.index + b0)
            ax.set_xlabel("year")
            ax.set_ylabel(f"annual mean({y_col})")
            ax.set_title("Annual mean + linear trend (reference)")
            st.pyplot(fig, clear_figure=True)

            st.write({"years": int(len(annual)), "slope_per_year": float(b1)})
        else:
            st.info("ì—°ë„ë³„ ì§‘ê³„ í›„ ì¶”ì„¸ ê³„ì‚°ì— í•„ìš”í•œ í‘œë³¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")

else:
    st.info("ì‹œê°„ ì»¬ëŸ¼ê³¼ ìˆ˜ì¹˜í˜• ë¶„ì„ ëŒ€ìƒì„ ì„ íƒí•˜ë©´ ì‹œê³„ì—´/ê³„ì ˆì„±/ì´ìƒ(anomaly) ë¶„ì„ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

# -----------------------------
# AI report
# -----------------------------
st.subheader("4) AI ì¸ì‚¬ì´íŠ¸/ë³´ê³ ì„œ (ì„ íƒ)")

if not enable_ai:
    st.info("ì™¼ìª½ì—ì„œ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê³  'AI ì¸ì‚¬ì´íŠ¸ ì‚¬ìš©'ì„ ì¼œë©´ í™œì„±í™”ë©ë‹ˆë‹¤.")
else:
    if st.button("ğŸ§  AI ë¶„ì„ ë³´ê³ ì„œ ìƒì„± (êµ¬ì¡°í™” ì¶œë ¥)"):
        if not api_key:
            st.error("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            with st.spinner("AI ë³´ê³ ì„œ ìƒì„± ì¤‘..."):
                report = ai_report(
                    df=dff,
                    eda=eda,
                    domain_context=domain_context,
                    date_col=date_col,
                    y_col=y_col,
                    group_col=group_col,
                    model_name=model_name,
                    api_key=api_key,
                    user_requirements=user_requirements
                )

            st.success("ì™„ë£Œ!")

            st.markdown("### í•œ ì¤„ ìš”ì•½")
            st.write(report["one_line_summary"])

            st.markdown("### í•µì‹¬ ê²°ê³¼")
            for i, f in enumerate(report["key_findings"], 1):
                with st.expander(f"{i}. {f['title']}"):
                    st.markdown(f"**ê·¼ê±°**: {f['evidence']}")
                    st.markdown(f"**ì˜í–¥/ì˜ë¯¸**: {f['impact']}")
                    st.markdown(f"**ë‹¤ìŒ ë‹¨ê³„**: {f['next_step']}")

            st.markdown("### ë°ì´í„° í’ˆì§ˆ ê²½ê³ ")
            st.write(report["data_quality_warnings"])

            st.markdown("### í†µê³„ì  ìœ ì˜ì‚¬í•­")
            st.write(report["statistical_notes"])

            st.markdown("### ì¶”ì²œ ëª¨ë¸/ë¶„ì„ í”„ë ˆì„")
            st.write(report["recommended_models"])

            st.markdown("### Executive Report (Markdown)")
            st.markdown(report["executive_report_md"])

            st.download_button(
                "ğŸ“¥ AI ë³´ê³ ì„œ(JSON) ë‹¤ìš´ë¡œë“œ",
                data=json.dumps(report, ensure_ascii=False, indent=2),
                file_name="ai_environment_report.json",
                mime="application/json"
            )

st.caption("Tip: ë°°í¬ í™˜ê²½ì—ì„œëŠ” secrets ê´€ë¦¬(OPENAI_API_KEY)ì™€ ë°ì´í„° ë³´ì•ˆ(PII ë§ˆìŠ¤í‚¹/ìƒ˜í”Œë§)ì„ ê¼­ í™•ì¸í•˜ì„¸ìš”.")
