import os
import json
import re
from typing import Optional, Dict, Any, Tuple

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt

# OpenAI is optional (app still runs without it)
try:
    from openai import OpenAI
except Exception:
    OpenAI = None


# -----------------------------
# Page config
# -----------------------------
st.set_page_config(page_title="ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ AI í™˜ê²½ë¶„ì„", layout="wide")
st.title("ğŸŒ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ AI í™˜ê²½ ë°ì´í„° ë¶„ì„ (Streamlit)")
st.caption("ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥ â†’ ë²„íŠ¼ ì‹¤í–‰ â†’ í•´ë‹¹ ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•œ ë¶„ì„/ë³´ê³ ì„œ ìƒì„±")


# -----------------------------
# Helpers: secrets/env
# -----------------------------
def get_setting(key: str, default: str = "") -> str:
    if key in st.secrets:
        return str(st.secrets.get(key, default))
    return os.getenv(key, default)


# -----------------------------
# Helpers: PII masking
# -----------------------------
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE_RE = re.compile(r"(\+?\d{1,3}[-\s]?)?(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{4})")

def mask_pii_text(s: str) -> str:
    s = EMAIL_RE.sub("[EMAIL]", s)
    s = PHONE_RE.sub("[PHONE]", s)
    return s

def mask_pii_df(df: pd.DataFrame, max_rows: int = 200) -> pd.DataFrame:
    out = df.copy()
    # keep small sample to reduce cost and protect privacy
    out = out.head(max_rows)
    obj_cols = out.select_dtypes(include="object").columns.tolist()
    for c in obj_cols:
        out[c] = out[c].astype(str).map(mask_pii_text)
    return out


# -----------------------------
# Data loading
# -----------------------------
@st.cache_data
def load_file(file) -> pd.DataFrame:
    name = file.name.lower()
    if name.endswith(".csv"):
        return pd.read_csv(file)
    if name.endswith(".xlsx") or name.endswith(".xls"):
        return pd.read_excel(file)
    raise ValueError("CSV ë˜ëŠ” Excelë§Œ ì§€ì›í•©ë‹ˆë‹¤.")


def infer_datetime(df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[str]]:
    out = df.copy()

    # Already datetime?
    for c in out.columns:
        if np.issubdtype(out[c].dtype, np.datetime64):
            return out, c

    cols = {c.lower(): c for c in out.columns}

    # Year/Month/Day
    if {"year", "month", "day"}.issubset(cols.keys()):
        y, m, d = cols["year"], cols["month"], cols["day"]
        out["date"] = pd.to_datetime(dict(year=out[y], month=out[m], day=out[d]), errors="coerce")
        return out, "date"

    # common single date col
    for key in ["date", "datetime", "time", "timestamp"]:
        if key in cols:
            c = cols[key]
            out[c] = pd.to_datetime(out[c], errors="coerce")
            return out, c

    return out, None


def numeric_cols(df: pd.DataFrame):
    return df.select_dtypes(include=[np.number]).columns.tolist()


def make_eda(df: pd.DataFrame) -> Dict[str, Any]:
    meta = []
    for c in df.columns:
        meta.append({
            "name": c,
            "dtype": str(df[c].dtype),
            "missing": int(df[c].isna().sum()),
            "n_unique": int(df[c].nunique(dropna=True))
        })

    num = numeric_cols(df)
    desc = {}
    if num:
        d = df[num].describe().T
        desc = d[["count","mean","std","min","25%","50%","75%","max"]].replace({np.nan: None}).to_dict()

    return {
        "shape": list(df.shape),
        "columns": meta,
        "numeric_describe": desc
    }


# -----------------------------
# Simple plots
# -----------------------------
def plot_time_series(df: pd.DataFrame, date_col: str, y_col: str, group_col: Optional[str] = None):
    fig, ax = plt.subplots()
    d = df.dropna(subset=[date_col, y_col]).sort_values(date_col)
    if group_col and group_col in d.columns:
        for g, gdf in d.groupby(group_col):
            ax.plot(gdf[date_col], gdf[y_col], label=str(g))
        ax.legend()
    else:
        ax.plot(d[date_col], d[y_col])
    ax.set_xlabel(date_col)
    ax.set_ylabel(y_col)
    ax.set_title(f"{y_col} over time")
    st.pyplot(fig, clear_figure=True)


# -----------------------------
# OpenAI: Structured Output schema
# -----------------------------
INSIGHT_SCHEMA = {
    "name": "requirements_based_report",
    "schema": {
        "type": "object",
        "additionalProperties": False,
        "properties": {
            "one_line_summary": {"type": "string"},
            "requirements_interpretation": {"type": "array", "items": {"type": "string"}},
            "key_findings": {"type": "array", "items": {"type": "string"}},
            "data_quality_warnings": {"type": "array", "items": {"type": "string"}},
            "statistical_notes": {"type": "array", "items": {"type": "string"}},
            "recommended_next_steps": {"type": "array", "items": {"type": "string"}},
            "executive_report_md": {"type": "string"}
        },
        "required": [
            "one_line_summary",
            "requirements_interpretation",
            "key_findings",
            "data_quality_warnings",
            "statistical_notes",
            "recommended_next_steps",
            "executive_report_md"
        ]
    }
}


def run_ai_report(
    df: pd.DataFrame,
    eda: Dict[str, Any],
    requirements: str,
    domain_context: str,
    date_col: Optional[str],
    y_col: Optional[str],
    group_col: Optional[str],
    api_key: str,
    model: str
) -> Dict[str, Any]:
    if OpenAI is None:
        raise RuntimeError("openai íŒ¨í‚¤ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. requirements.txt ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    client = OpenAI(api_key=api_key)

    masked_sample = mask_pii_df(df, max_rows=200).to_dict(orient="records")

    payload = {
        "domain_context": domain_context,
        "user_requirements": requirements,
        "selected_datetime_col": date_col,
        "selected_target_col": y_col,
        "selected_group_col": group_col,
        "eda_summary": eda,
        "masked_sample_rows": masked_sample
    }

    system = (
        "ë‹¹ì‹ ì€ í™˜ê²½/ê¸°í›„/í•´ì–‘ ë°ì´í„° ë¶„ì„ì„ ì´ê´„í•˜ëŠ” ìˆ˜ì„ ë¶„ì„ê°€ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ user_requirementsë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”. "
        "ê³¼ì¥ ê¸ˆì§€, ê·¼ê±° ê¸°ë°˜. ìƒê´€=ì¸ê³¼ ì˜¤ë¥˜ ê²½ê³ , ê³„ì ˆì„±/ìê¸°ìƒê´€/ê²°ì¸¡/ì´ìƒì¹˜/í‘œë³¸í¸í–¥ì˜ ê°€ëŠ¥ì„±ì„ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”. "
        "ì¶œë ¥ì€ ë°˜ë“œì‹œ ì£¼ì–´ì§„ JSON Schemaë¥¼ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤. "
        "executive_report_mdëŠ” 1~2í˜ì´ì§€ ë¶„ëŸ‰ì˜ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œë¡œ ì‘ì„±í•˜ì„¸ìš”."
    )

    resp = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)}
        ],
        text={
            "format": {
                "type": "json_schema",
                "json_schema": INSIGHT_SCHEMA
            }
        }
    )

    return json.loads(resp.output_text)


# -----------------------------
# Sidebar: inputs (requirements included)
# -----------------------------
with st.sidebar:
    st.header("âš™ï¸ ì‹¤í–‰ ì„¤ì •")

    domain_context = st.text_area(
        "ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸(ë¶„ì„ ëª©ì /ë°°ê²½)",
        value="í™˜ê²½/ê¸°í›„ ê´€ì¸¡ ë°ì´í„°(ì˜ˆ: í•´ë¹™, ì˜¨ë„, ê°•ìˆ˜)ì—ì„œ ì¥ê¸° ì¶”ì„¸ì™€ ê³„ì ˆì„±ì„ ì ê²€í•˜ê³  ì •ì±…ì  ì‹œì‚¬ì ì„ ë„ì¶œ",
        height=110
    )

    requirements = st.text_area(
        "ìš”êµ¬ ì‚¬í•­(ì—¬ê¸°ì— ì…ë ¥í•œ ë‚´ìš©ì´ AI ë³´ê³ ì„œì— ë°˜ì˜ë¨)",
        value=(
            "- ê²°ê³¼ëŠ” ì •ì±… ì œì–¸ ì¤‘ì‹¬ìœ¼ë¡œ\n"
            "- ê²°ì¸¡ì¹˜ ì²˜ë¦¬/í‘œë³¸í¸í–¥/ìê¸°ìƒê´€/ê³„ì ˆì„± í•œê³„ë¥¼ ë°˜ë“œì‹œ ì–¸ê¸‰\n"
            "- í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 5ê°œ ì´ë‚´ë¡œ ìš”ì•½\n"
            "- ë‹¤ìŒ ì‹¤í–‰(ì¶”ê°€ ë¶„ì„/ê²€ì •/ì‹œê°í™”) 3ê°œ ì œì•ˆ"
        ),
        height=170
    )

    st.subheader("OpenAI (ì„ íƒ)")
    api_key_default = get_setting("OPENAI_API_KEY", "")
    model_default = get_setting("OPENAI_MODEL", "gpt-4.1-mini")

    api_key = st.text_input("OPENAI_API_KEY", value=api_key_default, type="password")
    model = st.text_input("MODEL", value=model_default)

    st.divider()
    st.subheader("ë°ì´í„°")
    uploaded = st.file_uploader("CSV/Excel ì—…ë¡œë“œ", type=["csv", "xlsx", "xls"])
    use_sample = st.checkbox("ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©(seaice.csv)", value=(uploaded is None))


# -----------------------------
# Load dataset
# -----------------------------
df = None
if uploaded is not None:
    df = load_file(uploaded)
elif use_sample:
    sample_path = os.path.join(os.path.dirname(__file__), "seaice.csv")
    if os.path.exists(sample_path):
        df = pd.read_csv(sample_path)
    else:
        st.error("ìƒ˜í”Œ seaice.csvê°€ ì•± í´ë”ì— ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ì—…ë¡œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        st.stop()

if df is None:
    st.info("ì™¼ìª½ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    st.stop()

df, inferred_dt = infer_datetime(df)

# -----------------------------
# EDA
# -----------------------------
st.subheader("1) ë°ì´í„° ìš”ì•½ (EDA)")
eda = make_eda(df)

c1, c2, c3 = st.columns(3)
with c1:
    st.metric("í–‰", eda["shape"][0])
with c2:
    st.metric("ì—´", eda["shape"][1])
with c3:
    st.metric("ê²°ì¸¡ì¹˜ ì´í•©", sum(x["missing"] for x in eda["columns"]))

with st.expander("ì—´ ë©”íƒ€ì •ë³´"):
    st.dataframe(pd.DataFrame(eda["columns"]).sort_values("missing", ascending=False), use_container_width=True)

if st.checkbox("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", value=True):
    st.dataframe(df.head(50), use_container_width=True)

# -----------------------------
# Analysis selection
# -----------------------------
st.subheader("2) ë¶„ì„ ì„¤ì •")
all_cols = df.columns.tolist()
num = numeric_cols(df)

colA, colB, colC = st.columns([1, 1, 1])
with colA:
    date_col = st.selectbox("ì‹œê°„ ì»¬ëŸ¼", options=[None] + all_cols, index=(1 if inferred_dt in all_cols else 0))
with colB:
    y_col = st.selectbox("ë¶„ì„ ëŒ€ìƒ(ìˆ˜ì¹˜í˜•)", options=[None] + num, index=(1 if len(num) else 0))
with colC:
    group_candidates = [c for c in all_cols if df[c].dtype == "object"][:50]
    group_col = st.selectbox("ê·¸ë£¹ ì»¬ëŸ¼(ì„ íƒ)", options=[None] + group_candidates, index=(1 if "hemisphere" in group_candidates else 0))

dff = df.copy()
if date_col and date_col in dff.columns:
    dff[date_col] = pd.to_datetime(dff[date_col], errors="coerce")
    dff = dff.dropna(subset=[date_col])

# -----------------------------
# Plots
# -----------------------------
st.subheader("3) ì‹œê°í™”")
if date_col and y_col:
    plot_time_series(dff, date_col, y_col, group_col=group_col)
else:
    st.info("ì‹œê°„ ì»¬ëŸ¼ê³¼ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ë©´ ì‹œê³„ì—´ ê·¸ë˜í”„ê°€ ìƒì„±ë©ë‹ˆë‹¤.")

# -----------------------------
# Run button: requirements-driven execution
# -----------------------------
st.subheader("4) ìš”êµ¬ ì‚¬í•­ ê¸°ë°˜ ì‹¤í–‰")

st.markdown("ì•„ë˜ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ **ìš”êµ¬ ì‚¬í•­(requirements)**ì„ í¬í•¨í•´ AIê°€ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
run = st.button("ğŸš€ ìš”êµ¬ ì‚¬í•­ ë°˜ì˜ AI ë³´ê³ ì„œ ìƒì„±")

if run:
    if not api_key:
        st.error("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. (Streamlit secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥)")
        st.stop()
    with st.spinner("AI ë³´ê³ ì„œ ìƒì„± ì¤‘..."):
        report = run_ai_report(
            df=dff,
            eda=eda,
            requirements=requirements,
            domain_context=domain_context,
            date_col=date_col,
            y_col=y_col,
            group_col=group_col,
            api_key=api_key,
            model=model
        )

    st.success("ì™„ë£Œ!")

    st.markdown("### âœ… í•œ ì¤„ ìš”ì•½")
    st.write(report["one_line_summary"])

    st.markdown("### ğŸ§¾ ìš”êµ¬ ì‚¬í•­ í•´ì„")
    st.write(report["requirements_interpretation"])

    st.markdown("### ğŸ“Œ í•µì‹¬ ê²°ê³¼")
    st.write(report["key_findings"])

    st.markdown("### âš ï¸ ë°ì´í„° í’ˆì§ˆ ê²½ê³ ")
    st.write(report["data_quality_warnings"])

    st.markdown("### ğŸ§  í†µê³„ì  ìœ ì˜ì‚¬í•­")
    st.write(report["statistical_notes"])

    st.markdown("### âœ… ë‹¤ìŒ ì‹¤í–‰(ê¶Œê³ )")
    st.write(report["recommended_next_steps"])

    st.markdown("### ğŸ“„ Executive Report (Markdown)")
    st.markdown(report["executive_report_md"])

    st.download_button(
        "ğŸ“¥ AI ë³´ê³ ì„œ(JSON) ë‹¤ìš´ë¡œë“œ",
        data=json.dumps(report, ensure_ascii=False, indent=2),
        file_name="requirements_based_env_report.json",
        mime="application/json"
    )
